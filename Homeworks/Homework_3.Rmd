---
title: "Homework 3"
author: "Nicholas Esposito"
date: "2023-02-28"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Load 'tidyverse', 'ggplot2', 'class', 'gridExtra', 'MASS', 'knitr', 'e1071', and 'caret', 'reshape2' libraries, and set seed to '2'**

```{r, include = FALSE}
# Load libraries
library(tidyverse)
library(class)
library(ggplot2)
library(gridExtra)
library(MASS)
library(knitr)
library(reshape2)
library(e1071)
library(caret)

set.seed(2)
```

## Question 1

```{r}
# Read in data
q1 <- read.csv("Data/HW3/Crabs.csv")
q1_data <- data.frame(q1)
weight <- q1_data$weight
width <- q1_data$width
color <- q1_data$color
spine <- q1_data$spine
q1_y <- q1_data$y
q1_n <- nrow(q1_data)
```

**(i) Hold out a portion of the data set to be the testing data set, and use the rest as training data. Use support vector machines with a radial kernel and assess the sensitivity of results to the choice of the γ (gamma) tuning parameter. For this question, set the cost parameter to 1. Does the model begin to over fit to the training data?**

For smaller values of gamma, the model performs well, as evident by the decreasing error rates for the training and testing data sets. However, as the gamma values increase, the model is prone to over fitting. The test error begins to increase while the training error trends downwards, indicating that the model begins to over fit the data as gamma increases to larger values. NOTE: I used a log scale to better visualize the differences in error rate as gamma increased in value.

```{r}
# Split data into training/testing
samp <- sample(1:q1_n, floor(q1_n * 0.8), replace = FALSE) # 80 / 20, no replacement
q1_train <- q1_data[samp,]
q1_test <- q1_data[-samp,]

# Show sensitivity to gamma values by choosing range of values and comparing error rates
testError <- c()
trainError <- c()
gammaVals <- c(0.0001, 0.001, 0.01, 0.1, 1, 10, 100)

for(i in 1:length(gammaVals)) {
  q1_svm <- svm(y ~ ., 
                  data = q1_train,
                  kernel = "radial",
                  cost = 1,
                  scale = F,
                  gamma = gammaVals[i])
  
  # Run predictions on both the training and testing data
  predSVM_test <- 1*(predict(q1_svm, newdata = q1_test) > 0.5)
  predSVM_train <- 1*(predict(q1_svm, newdata = q1_train) > 0.5)
  
  testError[i] <- mean(predSVM_test != (q1_test$y))
  trainError[i] <- mean(predSVM_train != (q1_train$y))
}

svm_df <- reshape2::melt(data.frame(gammaVals, trainError, testError), id.var = "gammaVals")

# Plot SVM radial performance
svm_df %>%
  ggplot() + 
  geom_line(aes(x = gammaVals, y = value, color = variable)) +
  labs(title = "SVM Radial Performance", x = "Gamma Values", y = "Error Rates", color = "Dataset") +
  scale_x_continuous(trans = "log10") + # Use log to condense scale
  scale_color_discrete(labels = c("Train Error", "Test Error"))
```

**(ii) Let’s return to the KNN algorithm for classification. For this problem, do not use the hold out data from the previous two parts, i.e. the training data is your entire data set. I want you to run 10-fold cross validation on the KNN algorithm to find the optimal value of K in this data set. You must code this by hand. Do not use any pre-existing cross validation codes that are built into R to find the optimal value of K. For this question, please paste your R code for running the 10-fold cross validation along with your findings.**

The optimal k after running 10-fold cross validation on KNN is ~20, as this is where the error rate is at a minimum. In the next problem, using a built in tuning function for KNN, this value of k is verified. The error rate slowly climbs as k increases in value, peaking at ~0.38. There is a difference of ~0.1 between the minimum error rate found at the optimal k and the peak error rate of the algorithm.

```{r}
# Use 1/10th of dataset, q1_data, as testing and 9/10th as training (10 groups)
k_folds <- 10
k_vals <- seq(5,100,1) # Test values of K
knn_error_fold <- c()
knn_error <- c()
groups <- cut(1:q1_n, breaks = 10, labels = F)
k_itr <- 1

for(k_v in k_vals){
  
  # Clear vector to store error rates per fold
  knn_error_fold <- c()
  
  for (k_f in 1:k_folds) {
    # Split the data into training/testing
    testIndex <- which(groups == k_f)
  
    knn_train <- q1_data[-testIndex,]
    knn_test <- q1_data[testIndex,]
  
    # Store training/testing y values and set them to NULL is training/testing data set
    knn_train_y <- knn_train$y
    knn_test_y <- knn_test$y
  
    knn_train$y <- NULL
    knn_test$y <- NULL
  
    knn_pred <- knn(train = knn_train, 
                                 test = knn_test, cl = knn_train_y, k = k_v) # Run knn
  
    knn_error_fold[k_f] <- mean(knn_pred != knn_test_y)
  }
  
  # Store knn error rate for the current k
  knn_error[k_itr] <- mean(knn_error_fold)
  k_itr <- k_itr + 1
}

knn_errors_plot <- data.frame(k_vals, knn_error)

knn_errors_plot %>%
  ggplot() + 
  geom_line(aes(x = k_vals, y = knn_error)) +
  labs(title = "KNN Performance", x = "k-Values", y = "Error Rate")
```

**(iii) In this question, we will compare the performance of all of the classification algorithms that we have used so far. This includes logistic regression, LDA, QDA, KNN, SVMs with a polynomial kernel, and SVMs with a radial kernel. For any method that has tuning parameters, select them using cross-validation. Note for this question, you may use built in cross-validation functions (such as tune() for SVMs) for all algorithms. I want you to randomly select 25 subjects in the data to be the validation data and the rest to be your training data. Apply each algorithm on the training data and use them to classify the testing data outcomes. Keep track of the classification error rate for each algorithm. Do this 100 times, where each time you randomly pick 25 observations to be the testing data.**

**NOTE:** In practice, we would want to tune the KNN and SVM parameters with each iteration. However, in the interest of time, each method will be tuned prior to the trial iterations. Then, with each iteration, the methods will be fitted with the new training and testing (25 observations) data, according to the parameters found with the previous training and testing sets.

*(a) Which algorithm has the best performance, on average across the 100 testing data sets?*

On average across the 100 testing data sets, SVM with a radial kernel performs the best with an error rate of 0.2973. This method only slightly outperforms the other methods, which all have an an average error rate around 0.3. QDA has the highest error rate at 0.3377.

*(b) Make a boxplot that shows the distribution of error rates for each estimator across the 100 data sets. This plot should look similar to the plot on slide 53 of lecture 5. Comment on your findings.*

SVM with a radial kernel has the most compact distribution of error rates, while QDA has the largest spread of such values. As mentioned, QDA has the highest average error rate. KNN, SVM with a polynomial kernel, and LDA have the lowest minimum error rates, although their average error rates slightly exceed that of SVM with a radial kernel. 

```{r}
# Keep track of the error for each approach
numTrials <- 1000
errorMat <- matrix(NA, numTrials, 6)

# Tune KNN and both SVMs with previous training data

# KNN
trctrl <- trainControl(method = "repeatedcv", number = 10)
q1_knn_tune <- train(as.factor(y) ~ .,
                     data = q1_train,
                     method = "knn",
                     trControl = trctrl,
                     tuneLength = 10)
q1_knn_param <- q1_knn_tune$bestTune$k

# SVM - Polynomial
q1_svm_poly <- tune(svm,
                    as.factor(y) ~ .,
                    data = q1_train,
                    kernel = "polynomial",
                    ranges=list(cost=c(0.01, 1, 5, 10, 100),
                                degree=c(1,2))) 
q1_svm_poly_param <- q1_svm_poly$best.parameters


# SVM - Radial
q1_svm_rad <- tune(svm,
                   as.factor(y) ~ .,
                   data = q1_train,
                   kernel = "radial",
                   ranges=list(cost=c(0.01, 1, 5, 10, 100),
                               gamma=c(0.001, 0.01, 0.1, 1))) 
q1_svm_rad_param <- q1_svm_rad$best.parameters

for(n in 1:numTrials){
  
  # Pick 25 random observations to be testing data
  sampIndex <- sample(1:q1_n, 25, replace = FALSE)
  
  q1_iii_test <- q1_data[sampIndex,]
  q1_iii_train <- q1_data[-sampIndex,]
  
  # Logistic regression
  q1_logit <- glm(y ~ ., 
                  data = q1_iii_train,
                  family = binomial)
  
  test_pred_logit <- 1*(predict(q1_logit, newdata = q1_iii_test, type="response") > 0.5)
  
  errorMat[n,1] <- mean(test_pred_logit != (q1_iii_test$y))
  
  # LDA
  q1_lda <- lda(y ~ ., 
                data = q1_iii_train)
  
  test_pred_lda <- as.numeric(predict(q1_lda, 
                                      newdata = q1_iii_test)$class) - 1
  
  errorMat[n,2] <- mean(test_pred_lda != (q1_iii_test$y))
  
  # QDA
  q1_qda <- qda(y ~ ., 
                data = q1_iii_train)
  
  test_pred_qda <- as.numeric(predict(q1_qda, 
                                      newdata = q1_iii_test)$class) - 1
  errorMat[n,3] <- mean(test_pred_qda != (q1_iii_test$y))
  
  # KNN
  
  # Store training/testing y values and set them to NULL is training/testing data set
  knn_iii_train <- q1_iii_train
  knn_iii_test <- q1_iii_test
  
  q1_knn_train_y <- knn_iii_train$y
  q1_knn_test_y <- knn_iii_test$y
  
  knn_iii_train$y <- NULL
  knn_iii_test$y <- NULL
  
  q1_knn_fit <- knn(train = knn_iii_train, 
                    test = knn_iii_test, cl = q1_knn_train_y, k = q1_knn_param) # Run KNN
  
  errorMat[n,4] <- mean(q1_knn_fit != (q1_knn_test_y))
  
  # SVMs w/ Polynomial Kernel
  q1_svm_poly_fit <- svm(as.factor(y) ~ ., 
                         data = q1_iii_train,
                         kernel = "polynomial",
                         cost = q1_svm_poly_param$cost,
                         scale = F,
                         degree = q1_svm_poly_param$degree)
  
  test_pred_svm_poly <- (predict(q1_svm_poly_fit, newdata = q1_iii_test))
  
  errorMat[n,5] <- mean(test_pred_svm_poly != (q1_iii_test$y))
  
  # SVMs w/ Radial Kernel
  q1_svm_rad_fit <- svm(as.factor(y) ~ ., 
                        data = q1_iii_train,
                        kernel = "radial",
                        cost = q1_svm_rad_param$cost,
                        scale = F,
                        gamma = q1_svm_rad_param$gamma)
  
  test_pred_svm_rad <- (predict(q1_svm_rad_fit, newdata = q1_iii_test))
  
  errorMat[n,6] <- mean(test_pred_svm_rad != (q1_iii_test$y))
}

# Find mean error rate of each method
mean_logit <- mean(errorMat[,1])
mean_lda <- mean(errorMat[,2])
mean_qda <- mean(errorMat[,3])
mean_knn <- mean(errorMat[,4])
mean_svm_poly <- mean(errorMat[,5])
mean_svm_rad <- mean(errorMat[,6])

# Print values
q1_out <- cbind(mean_logit, mean_lda, mean_qda, mean_knn, mean_svm_poly, mean_svm_rad)
colnames(q1_out) <- c("Logistic", "LDA", "QDA", "KNN", "SVM-P", "SVM-R")
kable(q1_out, digits = 4, align = "l")

# Boxplot
boxplot(errorMat, names = c("Logistic", "LDA", "QDA", "KNN", "SVM-P", "SVM-R"), 
        main = "Comparing Error Rates")
```

# Question 2

**(i) In this case we know the distribution of Xi. What is the value of q0.8?**

Since we have a uniform distribution of 100 values on the interval [0,10], the value of q0.8 is 8. This would give us the probability expression P(Xi < 8) = 0.8, meaning 80% of our data (80 values) will lie below 8. 

**(ii) Suppose we don’t know the distribution of Xi. What is a good estimator of q0.8?**

A good estimator for q0.8, assuming we do not know the distribution of Xi, can be found by sorting the values in the data set in increasing order and extracting the 80th value. Since we have 100 values distributed on the interval [0,10], and we want to find a value in which 0.8 (80%) of the data lies below, we can look for the 80th percentile/quantile. When sorted in ascending order, the 80th-indexed value represents the 80th percentile, so it serves as a good estimator for q0.8. 

**(iii) Simulate one data set as above. Construct an estimate and confidence interval for q0.8 using the bootstrap.**

```{r}
set.seed(2)

# Construct a data set uniformly distributed on [0,10]
q2_data_iii <- runif(100, 0, 10)

# Keep track of 1000 bootstrap estimates of the mean
boot <- 1000
q2_vals <- rep(NA, boot)

# Perform bootstrap with resampling
for(b in 1:boot){
  sampIndex <- sample(1:100, 100, replace = TRUE)
  q2_vals[b] <- quantile(q2_data_iii[sampIndex], 0.8)
}

# Find standard error of the bootstrap
stdError <- sd(q2_vals)

estVal <- mean(q2_vals)

# Construct confidence interval
upperLim <- estVal + 1.96*stdError
lowerLim <- estVal - 1.96*stdError

# Print values
q2_iii_out <- cbind(estVal, lowerLim, upperLim)
colnames(q2_iii_out) <- c("Est.", "Lower", "Upper")
kable(q2_iii_out, digits = 4, align = "l")
```

**(iv) Now run a simulation study to assess the performance of your bootstrapped confidence intervals by finding the 95% interval coverage provided by your intervals. I would like you to assess the performance of both the percentile method for constructing confidence intervals, as well as the method that finds the standard error of the estimator and then uses the standard confidence interval formula that adds or subtracts 1.96 times the standard error. In what percentage of your simulations do the bootstrap intervals cover the true parameter?**

The standard error method and the percentile method have very similar performance. The standard error method captures the true value of q0.8 at a rate of ~0.94 (expected for a 95% confidence interval), while the percentile method captures the true value at a rate of ~0.96. The percentile method has a slightly wider confidence interval in this case, so it is expected that its rate is marginally better than that of the standard error method.

```{r}
simStudy <- function(q, est){
  numTrials <- 100
  cov <- matrix(NA, numTrials, 2)
  
  for(n in 1:numTrials){
    # Construct a data set uniformly distributed on [0,10]
    q2_data_iv <- runif(100, 0, 10)
    
    # Keep track of 1000 bootstrap estimates of the mean
    numBoot <- 1000
    q2_boot_vals <- rep(NA, numBoot)
  
    for(b in 1:numBoot){
      sampIndexBoot <- sample(1:100, 100, replace = TRUE)
      q2_boot_vals[b] <- quantile(q2_data_iv[sampIndexBoot], q)
    }
  
    # Find standard error of the bootstrap
    newStdError <- sd(q2_boot_vals)
  
    newEstVal <- mean(q2_boot_vals)
  
    # Construct confidence interval with 1.96*SE
    stdLowerLim <- newEstVal - (1.96*newStdError)
    stdUpperLim <- newEstVal + (1.96*newStdError)
    
    cov[n,1] <- 1*(stdLowerLim < est &&
                       stdUpperLim > est)
    
    # Find 0.025 and 0.975 quantiles for percentile confidence interval method
    q2_boot_vals_sorted <- sort(q2_boot_vals)
    qLowerLim <- quantile(q2_boot_vals_sorted, 0.025)
    qUpperLim <- quantile(q2_boot_vals_sorted, 0.975)
    
    cov[n,2] <- 1*(qLowerLim < est &&
                       qUpperLim > est)
  }
  
  # Find rate in which each bootstrap interval contains the true parameter, 8
  stdRate <- mean(cov[,1])
  qRate <- mean(cov[,2])
  
  # Print values
  q2_iv_out <- cbind(stdRate, qRate)
  colnames(q2_iv_out) <- c("SE Method", "Percentile Method")
  kable(q2_iv_out, digits = 4, align = "l")
}
```

```{r}
# Confidence Interval for q0.8
set.seed(2)
simStudy(0.8, 8)
```

**(v) Run the same simulation as in (iv) but now perform inference on q0.99. Comment on any differences you find, and explain why you think there are differences.**

The standard error method has a similar performance in finding q0.99 as it did in finding q0.8, dropping slightly to ~0.88. However, the performance of the percentile method falls significantly to ~0.57. This result is expected. The limits of the standard error method's confidence interval are symmetric about the estimated parameter, 9.9. Thus, we expect better performance for this method in capturing the true value of q0.99. However, the percentile method's confidence interval is not symmetric about the estimated parameter. Instead, it is centered on the 50th percentile (q0.5), and thus is less successful in capturing the true value of q0.99, which is located in the tail of such a distribution. 

```{r}
# Confidence Interval for q0.99
set.seed(2)
simStudy(0.99, 9.9)
```

## Question 3

![](Data/HW3/HW3Q3.png){width=70%, height=50%}